{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e87bc66d164559a134bf7a744a75c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/187952 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd917009b4174abcb95af12020dc6d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/46988 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab16761d39b74dcbbeb9f0aa4dc87d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/16003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from birdset.datamodule import DatasetConfig\n",
    "from birdset.datamodule.birdset_datamodule import BirdSetDataModule\n",
    "from birdset.datamodule.components.transforms import BirdSetTransformsWrapper\n",
    "\n",
    "transforms = BirdSetTransformsWrapper(\n",
    "    task='multiclass',\n",
    "    sampling_rate = 16000,\n",
    "    model_type = 'waveform',\n",
    ")\n",
    "\n",
    "dm = BirdSetDataModule(\n",
    "    dataset= DatasetConfig(\n",
    "        data_dir='B:\\DLL\\Datasets',\n",
    "        dataset_name='XCM',\n",
    "        hf_path='DBD-research-group/BirdSet',\n",
    "        hf_name='XCM',\n",
    "        n_workers=8,\n",
    "        val_split=0.2,\n",
    "        task=\"multiclass\",\n",
    "        classlimit=500,\n",
    "        eventlimit=5,\n",
    "        sampling_rate=16000,\n",
    "    ),\n",
    "    transforms=transforms\n",
    ")\n",
    "\n",
    "dm.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 80000])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "dm.setup(stage=\"fit\")\n",
    "train_loader = dm.train_dataloader()\n",
    "batch = next(iter(train_loader))\n",
    "# get shape of the batch\n",
    "print(batch[\"input_values\"].shape)\n",
    "print(batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  9,   1, 316, 367,  52, 116, 407,  30,   7,  22, 369, 325, 327,  68,\n",
      "        110, 295, 325, 123, 324,  60, 234, 388,  45, 306, 170, 338, 131, 313,\n",
      "        170,  79, 266, 312])\n",
      "411\n"
     ]
    }
   ],
   "source": [
    "print(batch[\"labels\"])\n",
    "print(dm.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type hubert to instantiate a model of type wav2vec2. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/hubert-base-ls960 and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Model\n",
    "import torch\n",
    "basemodel = Wav2Vec2Model.from_pretrained(\"facebook/hubert-base-ls960\", torch_dtype=torch.float16, attn_implementation=None).to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2Model(\n",
      "  (feature_extractor): Wav2Vec2FeatureEncoder(\n",
      "    (conv_layers): ModuleList(\n",
      "      (0): Wav2Vec2GroupNormConvLayer(\n",
      "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
      "        (activation): GELUActivation()\n",
      "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
      "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "      (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
      "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (feature_projection): Wav2Vec2FeatureProjection(\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (projection): Linear(in_features=512, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): Wav2Vec2Encoder(\n",
      "    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
      "      (conv): ParametrizedConv1d(\n",
      "        768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
      "        (parametrizations): ModuleDict(\n",
      "          (weight): ParametrizationList(\n",
      "            (0): _WeightNorm()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (padding): Wav2Vec2SamePadLayer()\n",
      "      (activation): GELUActivation()\n",
      "    )\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Wav2Vec2EncoderLayer(\n",
      "        (attention): Wav2Vec2SdpaAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Wav2Vec2FeedForward(\n",
      "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(basemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modelwrapper(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model, num_classes, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.model = model\n",
    "        self.lastConv = torch.nn.Conv1d(249, 1, kernel_size=1, dtype=torch.float16)\n",
    "        self.linear = torch.nn.Linear(512, num_classes, dtype=torch.float16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x.squeeze().half()\n",
    "        output = self.model(input, return_dict=True)\n",
    "        output = output[\"extract_features\"]\n",
    "        print(\"out:\", output.shape)\n",
    "        return torch.nn.functional.sigmoid(self.linear(self.lastConv(output).squeeze()))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setup(stage=\"fit\")\n",
    "train_loader = dm.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_classes = dm.num_classes\n",
    "\n",
    "model = Modelwrapper(basemodel, num_classes).to(\"cuda:1\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  torch.Size([32, 1, 80000])\n",
      "out: torch.Size([32, 249, 512])\n",
      "loss 6.015625\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        print(\"batch: \", batch[\"input_values\"].shape)\n",
    "        pred = model(batch[\"input_values\"].to(\"cuda:1\"))\n",
    "\n",
    "        loss = loss_func(pred, batch[\"labels\"].to(\"cuda:1\"))\n",
    "        losses.append(loss)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    print(f\"epoch {i}: trainloss {torch.mean(loss)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
